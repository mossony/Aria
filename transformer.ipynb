{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'soundfile'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "str(torchaudio.get_audio_backend())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T01:46:32.893252300Z",
     "start_time": "2023-08-02T01:46:29.779357100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Set the default device to the first available GPU (index 0)\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T01:46:40.011202600Z",
     "start_time": "2023-08-02T01:46:39.996607300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# # convert all mp3 to wav\n",
    "# genre_folder = r\"\"\n",
    "# i = 10\n",
    "#\n",
    "# for song in os.listdir(genre_folder):\n",
    "#     song_path = os.path.join(genre_folder, song)\n",
    "#     audio = AudioSegment.from_mp3(song_path)\n",
    "#     wav_file_path = os.path.join(genre_folder, str(i) + \".wav\")\n",
    "#     audio.export(wav_file_path, format=\"wav\")\n",
    "#     i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T01:46:41.567385300Z",
     "start_time": "2023-08-02T01:46:41.551346800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1620, 100])\n",
      "Y shape: torch.Size([1620, 3])\n"
     ]
    }
   ],
   "source": [
    "genre_to_i = {}\n",
    "i_to_genre = {}\n",
    "downsample_size = 2205 # get a sample every 441 samples, resulting 20 per second\n",
    "sample_rate = 44100\n",
    "segment_seconds = 5\n",
    "input_size = sample_rate // downsample_size * segment_seconds\n",
    "# target_size = sample_rate // downsample_size\n",
    "X = []\n",
    "Y_ = []\n",
    "\n",
    "num_genres = 0\n",
    "for genre in os.listdir(\"./assets\"):\n",
    "    genre_to_i[genre] = num_genres\n",
    "\n",
    "    for song in os.listdir(\"./assets/\" + genre):\n",
    "        if song.endswith(\".wav\"):\n",
    "            waveform = torchaudio.load(os.path.join(\"./assets\", genre, song))[0][0] # get the left channel waveform\n",
    "            for i in range(0, len(waveform) - sample_rate * segment_seconds, sample_rate * segment_seconds):\n",
    "                one_sec_tensor = waveform[i:i+sample_rate * segment_seconds]\n",
    "                one_sec_list = one_sec_tensor.tolist()[0::downsample_size]\n",
    "                X.append(one_sec_list)\n",
    "                Y_.append(num_genres)\n",
    "    num_genres += 1\n",
    "\n",
    "Y = torch.zeros((len(X), num_genres))\n",
    "for j in range(len(Y_)):\n",
    "    Y[j][Y_[j]] += 1\n",
    "X = torch.tensor(X)\n",
    "\n",
    "min_value = torch.min(X)\n",
    "max_value = torch.max(X)\n",
    "\n",
    "# Min-Max normalization\n",
    "X = (X - min_value) / (max_value - min_value)\n",
    "\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T01:47:38.500377700Z",
     "start_time": "2023-08-02T01:47:27.735247400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.4025, 0.1984, 0.7995, 0.6840, 0.1680, 0.3593, 0.7504, 0.6914, 0.3243,\n        0.3810, 0.6560, 0.5887, 0.0990, 0.2997, 0.6490, 0.5467, 0.3599, 0.3265,\n        0.6706, 0.4675, 0.1607, 0.0752, 0.7577, 0.8506, 0.2810, 0.3931, 0.7567,\n        0.5117, 0.0972, 0.5995, 0.8241, 0.5106, 0.1247, 0.1384, 0.1971, 0.6540,\n        0.6284, 0.2963, 0.5506, 0.7104, 0.6693, 0.0976, 0.4305, 0.9094, 0.9499,\n        0.7188, 0.6255, 0.3276, 0.5101, 0.9013, 0.2999, 0.3214, 0.8105, 0.7192,\n        0.0977, 0.5321, 0.6833, 0.4536, 0.1146, 0.8937, 0.5682, 0.3498, 0.1145,\n        0.6464, 0.8816, 0.2583, 0.2930, 0.5750, 0.7405, 0.1999, 0.4391, 0.6253,\n        0.5212, 0.4453, 0.4285, 0.6184, 0.3595, 0.4540, 0.3355, 0.7640, 0.2221,\n        0.0895, 0.7133, 0.7298, 0.1614, 0.4300, 0.1336, 0.4785, 0.8461, 0.7617,\n        0.0394, 0.5169, 0.7287, 0.2033, 0.2666, 0.6567, 0.7365, 0.2060, 0.6162,\n        0.8513], device='cuda:0')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[150]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T01:47:45.934022600Z",
     "start_time": "2023-08-02T01:47:45.919003200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in Xtr, Ytr: 1296\n",
      "Number of samples in Xdev, Ydev: 162\n",
      "Number of samples in Xte, Yte: 162\n"
     ]
    }
   ],
   "source": [
    "# split the dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assuming you have X and Y tensors with shapes:\n",
    "# X shape: torch.Size([4914, 100])\n",
    "# Y shape: torch.Size([4914, 3])\n",
    "\n",
    "# Define the percentages for train, dev, and test sets\n",
    "train_percent = 0.8\n",
    "dev_percent = 0.1\n",
    "test_percent = 0.1\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "num_samples = X.shape[0]\n",
    "num_train_samples = int(num_samples * train_percent)\n",
    "num_dev_samples = int(num_samples * dev_percent)\n",
    "num_test_samples = num_samples - num_train_samples - num_dev_samples\n",
    "\n",
    "# Create a dataset from X and Y\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "\n",
    "# Split the dataset into train, dev, and test sets\n",
    "train_dataset, dev_dataset, test_dataset = random_split(\n",
    "    dataset, [num_train_samples, num_dev_samples, num_test_samples]\n",
    ")\n",
    "\n",
    "# Create data loaders for each split\n",
    "batch_size = 32  # Set an appropriate batch size for training\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Check the number of samples in each split\n",
    "print(\"Number of samples in Xtr, Ytr:\", len(train_dataset))\n",
    "print(\"Number of samples in Xdev, Ydev:\", len(dev_dataset))\n",
    "print(\"Number of samples in Xte, Yte:\", len(test_dataset))\n",
    "\n",
    "def extract_input_target(dataset):\n",
    "    X = torch.stack([sample[0] for sample in dataset])\n",
    "    Y = torch.stack([sample[1] for sample in dataset])\n",
    "    return X, Y\n",
    "\n",
    "# Extract input and target for training dataset\n",
    "Xtr, Ytr = extract_input_target(train_dataset)\n",
    "\n",
    "# Extract input and target for development (validation) dataset\n",
    "Xdev, Ydev = extract_input_target(dev_dataset)\n",
    "\n",
    "# Extract input and target for test dataset\n",
    "Xte, Yte = extract_input_target(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T01:47:58.098456800Z",
     "start_time": "2023-08-02T01:47:58.056836500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Implementation of Transformer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T02:08:38.256211400Z",
     "start_time": "2023-08-02T02:08:38.241706200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
